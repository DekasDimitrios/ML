{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce63642cafb413e7903d83d2f2cd3637",
     "grade": false,
     "grade_id": "cell-f62db6dce1ed3f2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 2 - Decision Trees #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29d61ce286fdb8fd61c7f8e89a9e1339",
     "grade": false,
     "grade_id": "cell-dce2e73cee9a5017",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Welcome to your second assignment. This exercise gives you an introduction to [scikit-learn](https://scikit-learn.org/stable/). A simple but efficient machine learning library in Python. It also gives you a wide understanding on how decision trees work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50a108d2f1e1a1ee2fde80743c0543fe",
     "grade": false,
     "grade_id": "cell-83ca2b0456fb85db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After this assignment you will:\n",
    "- Be able to use the scikit-learn library and train your own model from scratch.\n",
    "- Be able to train and understand decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "396c39a0797964c378ebb90cf18a29de",
     "grade": false,
     "grade_id": "cell-2cef6d48eea484d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Always run this cell\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import unittest\n",
    "\n",
    "# USE THIS RANDOM VARIABLE TO PRODUCE THE SAME RESULTS\n",
    "RANDOM_VARIABLE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scikit-Learn and Decision Trees ##\n",
    "\n",
    "You are going to use the scikit-learn library to train a model for detecting breast cancer using the [Breast cancer wisconsin (diagnostic) dataset](https://scikit-learn.org/stable/datasets/index.html#breast-cancer-wisconsin-diagnostic-dataset) by training a model using [decision trees](https://scikit-learn.org/stable/modules/tree.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1** Load the breast cancer dataset using the scikit learn library and split the dataset into train and test set using the appropriate function. Use 30% of the dataset as the test set. Define as X the attributes and as y the target values. Do not forget to set the random_state parameter as the *RANDOM_VARIABLE* defined above. Use this variable for all the random_state parameters in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b873328ea05f6ef9c08827168c7b835",
     "grade": false,
     "grade_id": "cell-1f0c2f3918333cf6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "X,y = load_breast_cancer(return_X_y=True)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_VARIABLE)\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3603b2ba8916ffdad9e9c53f31546b4c",
     "grade": true,
     "grade_id": "cell-3f43c895ceaf57a9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Size of train set:{}\".format(len(y_train)))\n",
    "print(\"Size of test set:{}\".format(len(y_test)))\n",
    "print(\"Unique classes:{}\".format(len(set(y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62285a7bd3ab59718b89f7e09de0fea4",
     "grade": false,
     "grade_id": "cell-1ce621a108e76a15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected output**:\n",
    "Size of train set:398\n",
    "Size of test set:171\n",
    "Unique classes:2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2** Train two DecisionTree classifiers and report the F1 score. Use the information gain for the one classifier and the Gini impurity for the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17197b62614427a979fcbab7ed2734dd",
     "grade": false,
     "grade_id": "cell-a7fa1d29509eb2a1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "classifier_gini = DecisionTreeClassifier(criterion=\"gini\", random_state=RANDOM_VARIABLE).fit(X_train, y_train)\n",
    "classifier_igain = DecisionTreeClassifier(criterion=\"entropy\", random_state=RANDOM_VARIABLE).fit(X_train, y_train)\n",
    "\n",
    "prediction_gini = classifier_gini.predict(X_test)\n",
    "prediction_igain = classifier_igain.predict(X_test)\n",
    "\n",
    "f_measure_gini = f1_score(y_true=y_test, y_pred=prediction_gini)\n",
    "f_measure_igain = f1_score(y_true=y_test, y_pred=prediction_igain)\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d9aab4355c27c346f7e6548f233e758",
     "grade": true,
     "grade_id": "cell-09657a82bf4028c4",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"F-Measure Gini:{}\".format(f_measure_gini))\n",
    "print(\"F-Measure Information Gain:{}\".format(f_measure_igain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f3facbbef0dd8f25ad12bfec7c174818",
     "grade": false,
     "grade_id": "cell-b0d8630f3b764cf3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected output**:\n",
    "F-Measure Gini:0.9528301886792453\n",
    "F-Measure Information Gain:0.9724770642201834\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f2532168d16e8c9bffba3d7d8e1efce7",
     "grade": false,
     "grade_id": "cell-591ba122016b6db5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**1.3** Find the maximum depth reached by the tree that used the Gini impurity. Train multiple classifier by modifying the max_depth within the range from 1 to maximum depth and save the f1 scores to lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54cf257e90a3cb5877db81297bedd45c",
     "grade": false,
     "grade_id": "cell-31c58b6161a3907d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "depth = classifier_gini.get_depth()\n",
    "fscores_train = []\n",
    "fscores_test = []\n",
    "\n",
    "for i in range(1, depth + 1):\n",
    "    clf = DecisionTreeClassifier(criterion=\"gini\", random_state=RANDOM_VARIABLE, max_depth=i).fit(X_train, y_train)\n",
    "    fscores_train.append(f1_score(y_true=y_train, y_pred=clf.predict(X_train)))\n",
    "    fscores_test.append(f1_score(y_true=y_test, y_pred=clf.predict(X_test)))\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70a249937f2f690c6ce855debaed204c",
     "grade": true,
     "grade_id": "cell-0c300109423f53b9",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Fscores Train:{}\".format(fscores_train))\n",
    "print(\"Fscores Test:{}\".format(fscores_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3db472d2b9db7a42cc012cd96fdeb499",
     "grade": false,
     "grade_id": "cell-75789627f20d2c94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expected output**:\n",
    "Fscores Train:[0.9392712550607287, 0.9533468559837729, 0.9761904761904762, 0.996, 0.996, 0.9979959919839679, 1.0]\n",
    "Fscores Test:[0.9150943396226415, 0.9444444444444444, 0.9724770642201834, 0.9629629629629629, 0.9629629629629629, 0.9674418604651163, 0.9528301886792453]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bca7d4c160c767d27a09b4620d27d56e",
     "grade": false,
     "grade_id": "cell-5906e6d5efa70282",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**1.4** Compare the results from the train set with the results from the test set. What do you notice? Explain your findings. How are you going to choose the max_depth of your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "424ac10e4e22ca9e32207deee3bf0f57",
     "grade": true,
     "grade_id": "cell-c9c6ea0e40d98b83",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "As the depth is being increased, the training accuracy is steadily increasing until it reaches the perfect fit on the\n",
    "training data. For the testing accuracy, an improvement is being noticed until the depth of 3 is reached. After that,\n",
    "the testing accuracy is getting worse. This phenomenon can be easily described with a single word, overfitting. The\n",
    "final goal of the model is to be trained and achieve a good accuracy on the training data, but also generalize well.\n",
    "That being said, the best option for the model's max_depth seems to be equal to 3, for which the decision tree achieves\n",
    "0.97 accuracy during both the training and testing phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "217666fcc2e383d6f2c1904c9d6a71be",
     "grade": false,
     "grade_id": "cell-9ef42e6c90ea2ffe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.0 Pipelines ##\n",
    "\n",
    "**2.1** In this part of the exercise you are going to build a pipeline from scratch for a classification problem. Load the **income.csv** file and train a DecisionTree model that will predict the *income* variable. This dataset is a modification of the original Adult Income dataset found [here](http://archive.ics.uci.edu/ml/datasets/Adult). Report the f1-score and accuracy score of the test set found in **income_test.csv**. Your pipeline should be able to handle missing values and categorical features (scikit-learn's decision trees do not handle categorical values). You can preprocess the dataset as you like in order to achieve higher scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "152ab2dd6861b198b879a78ebadc4ee4",
     "grade": true,
     "grade_id": "cell-dd950ab2eb40d8a4",
     "locked": false,
     "points": 45,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Loading Datasets\n",
    "training_set = pd.read_csv(\"income.csv\", header=0)\n",
    "testing_set = pd.read_csv(\"income_test.csv\", header=0)\n",
    "\n",
    "cols = ['age','workclass','fnlwgt','education','education_num','marital-status','occupation',\n",
    "        'relationship','race','sex','capital-gain','capital-loss','hours-per-week','income']\n",
    "num_cols = ['age','fnlwgt','education_num',\"capital-gain\",\"capital-loss\",'hours-per-week']\n",
    "categorical_cols = ['workclass','marital-status','occupation','relationship','race','sex']\n",
    "target_feature = 'income'\n",
    "\n",
    "# Missing Values\n",
    "\n",
    "# Option 2: Drop the samples that have missing values.\n",
    "# training_set = training_set.dropna()\n",
    "# testing_set = testing_set.dropna()\n",
    "\n",
    "# Option 3: Fill the missing values with the most frequent one per column.\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n",
    "training_set = imputer.fit_transform(training_set)\n",
    "training_set = pd.DataFrame(training_set, columns=cols)\n",
    "\n",
    "# Convert target variable to 0 and 1\n",
    "training_set[target_feature] = training_set[target_feature].map({'<=50K': 0, '>50K': 1})\n",
    "testing_set[target_feature] = testing_set[target_feature].map({'<=50K': 0, '>50K': 1})\n",
    "\n",
    "training_set_labels = training_set[target_feature]\n",
    "testing_set_labels = testing_set[target_feature]\n",
    "\n",
    "training_set = training_set.drop(target_feature, axis=1)\n",
    "testing_set = testing_set.drop(target_feature, axis=1)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('normalization', MinMaxScaler())\n",
    "    # ('standardization', StandardScaler())\n",
    "])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num_pipeline', num_pipeline, num_cols),\n",
    "    ('1hot', OneHotEncoder(), categorical_cols)\n",
    "])\n",
    "\n",
    "transformer = full_pipeline.fit(training_set)\n",
    "\n",
    "X_train = pd.DataFrame(transformer.transform(training_set).toarray())\n",
    "X_test = pd.DataFrame(transformer.transform(testing_set).toarray())\n",
    "\n",
    "y_train = pd.DataFrame(training_set_labels)\n",
    "y_test = pd.DataFrame(testing_set_labels)\n",
    "\n",
    "# param_grid = [{'criterion': [\"gini\", \"entropy\"],\n",
    "#                'max_depth': list(range(1, 45)),\n",
    "#                'min_samples_split': [2, 3, 4, 5]\n",
    "#                }]\n",
    "#\n",
    "# grid_search = GridSearchCV(DecisionTreeClassifier(random_state = RANDOM_VARIABLE), param_grid, cv=10, scoring='f1', return_train_score=True, verbose=3)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# print(grid_search.best_params_)\n",
    "# CV Result: {'criterion': 'entropy', 'max_depth': 13, 'min_samples_split': 2}\n",
    "\n",
    "# Using the CV best parameters\n",
    "clf = DecisionTreeClassifier(random_state=RANDOM_VARIABLE, criterion='entropy', max_depth=13, min_samples_split=2)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "ftrScore = f1_score(y_true=y_train, y_pred=clf.predict(X_train))\n",
    "fteScore = f1_score(y_true=y_test, y_pred=clf.predict(X_test))\n",
    "acctrScore = accuracy_score(y_true=y_train, y_pred=clf.predict(X_train))\n",
    "accteScore = accuracy_score(y_true=y_test, y_pred=clf.predict(X_test))\n",
    "\n",
    "# print(\"Accuracy Train:{}\".format(acctrScore))\n",
    "# print(\"Accuracy Test:{}\".format(accteScore))\n",
    "# print(\"Fscores Train:{}\".format(ftrScore))\n",
    "# print(\"Fscores Test:{}\".format(fteScore))\n",
    "\n",
    "fScore = '0.6744918408245062'\n",
    "accScore = '0.8515181194906954'\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee9d4c2635307395bdef2efb941106ae",
     "grade": false,
     "grade_id": "cell-2c3327274958bbad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**2.2** Describe the process you followed to achieve the results above. Your description should include, but is not limited to the following\n",
    "- How do you handle missing values and why\n",
    "- How do you handle categorical variables and why\n",
    "- Any further preprocessing steps\n",
    "- How do you evaluate your model and how did you choose its parameters\n",
    "- Report any additional results and comments on your approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1aaf3ddda45b52c2e43089b082d030f1",
     "grade": true,
     "grade_id": "cell-80274fd09b80518c",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "For the handling of missing values, there are three main options: <br />\n",
    "\n",
    "> Option 1: Drop the features that have missing values.\n",
    "\n",
    "> Option 2: Drop the samples that have missing values.\n",
    "\n",
    "> Option 3: Fill the missing values with a constant value.\n",
    "\n",
    "Option 1 seems too extreme for our case since only 5% of the training data has missing values. Option 2 could be a\n",
    "viable solution, if the 5% of training data is not deemed as a significant percentage to be dropped. For option 3, and\n",
    "since the missing values correspond to categorical features, the most frequent value of each column is used.\n",
    "\n",
    "For the handling of categorical variables, the typical One Hot Encoding is used since there is no obvious reason why\n",
    "it should not be used.\n",
    "\n",
    "For the numerical valued features, both normalization and standardization could be deployed. The difference seems to be\n",
    "neglectable between the results of those two.\n",
    "\n",
    "In order to evaluate the model and make the parameter selection, the standard GridSearchCV was deployed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8cef3f333ab449ed91b81ea96695e712",
     "grade": false,
     "grade_id": "cell-555d20216f9bbec2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3.0 Common Issues ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.0** Run the following code to define a DecisionTreeModel and load the **income** dataset only with the numerical variables. Then, answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae0f57b86252cc38b02cac3d05e08bbf",
     "grade": false,
     "grade_id": "cell-d7f58621bad12aad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "columns = ['age','fnlwgt','education_num','hours-per-week',\"capital-loss\",\"capital-gain\",\"income\"]\n",
    "data = pd.read_csv('income.csv',usecols=columns)\n",
    "data_test = pd.read_csv('income_test.csv',usecols=columns)\n",
    "# Convert target variable to 0 and 1\n",
    "data[\"income\"] = data[\"income\"].map({ \"<=50K\": 0, \">50K\": 1 })\n",
    "data_test[\"income\"] = data_test[\"income\"].map({ \"<=50K\": 0, \">50K\": 1 })\n",
    "# Create X and y\n",
    "X_train = data.drop([\"income\"],axis=1)\n",
    "y_train = data['income'].values\n",
    "X_test = data_test.drop([\"income\"],axis=1)\n",
    "y_test = data_test['income'].values\n",
    "# Classifier\n",
    "classifier = DecisionTreeClassifier(min_samples_leaf=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3981752b539236e99415ab6e2cbea1f",
     "grade": false,
     "grade_id": "cell-9b18d6c4e381a9f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**3.1** Draw a learning curve for the classifer for the train and test set loaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12b88026c150b617074a5c06fea36b73",
     "grade": true,
     "grade_id": "cell-905e7dceeb4172c3",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def learning_curve(X_train,y_train,X_test,y_test,clf,metric): #Use any parameters you need\n",
    "    trainErrs = []\n",
    "    testErrs = []\n",
    "    slices = np.linspace(0, X_train.shape[0], 11, dtype=int)[1:]\n",
    "    for sliceIdx in slices:\n",
    "        model = clf.fit(X_train[:sliceIdx], y_train[:sliceIdx])\n",
    "        if metric == 'f1':\n",
    "            trainErrs.append(f1_score(y_true=y_train[:sliceIdx], y_pred=clf.predict(X_train[:sliceIdx])))\n",
    "            testErrs.append(f1_score(y_true=y_test, y_pred=clf.predict(X_test)))\n",
    "        elif metric == 'accuracy':\n",
    "            trainErrs.append(accuracy_score(y_true=y_train[:sliceIdx], y_pred=clf.predict(X_train[:sliceIdx])))\n",
    "            testErrs.append(accuracy_score(y_true=y_test, y_pred=clf.predict(X_test)))\n",
    "        else:\n",
    "            print(\"Wrong metric parameter used.\")\n",
    "\n",
    "    plt.plot(np.arange(10, 101, 10), trainErrs, label='training set')\n",
    "    plt.plot(np.arange(10, 101, 10), testErrs, label='test set')\n",
    "    plt.xlabel('Percentage of training set used')\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlim([0, 110])\n",
    "    plt.ylim([min(min(testErrs), min(trainErrs)) - 0.2, 0.2 + max(max(testErrs), max(trainErrs))])\n",
    "    plt.show()\n",
    "    return trainErrs, testErrs\n",
    "\n",
    "\n",
    "f1Errs = learning_curve(X_train,y_train,X_test,y_test,classifier,'f1')\n",
    "accErrs = learning_curve(X_train,y_train,X_test,y_test,classifier,'accuracy')\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "361d4753f3c8491a34ff55b6fa3a49b5",
     "grade": false,
     "grade_id": "cell-1f23f3e27600f019",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**3.2** Do you notice any problems with the classifier? If so, what can you do to change this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "baff993106fd2b655fd47d05c75ea4ce",
     "grade": true,
     "grade_id": "cell-d60d7e6175d184e9",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The produced learning curves show that the trained model suffers from high variance, and thus overfitting. In order\n",
    "to overcome this problem, model regularization should be attempted. Tuning the parameters of the model could help in\n",
    "that direction. The DecisionTreeClassifier is known for its overfitting nature. Letting the tree grow indefinitely\n",
    "will result in training data overadaption. To compat this, the model has to be stopped before overadapting to the data\n",
    "used for training by having limited depth, and a limited number of leafs in best-first fashion. Best nodes are defined\n",
    "as relative reduction in impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "747645b33cb4f5c14796504fac6bf3ce",
     "grade": false,
     "grade_id": "cell-89715acd6c51b332",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**3.3** Implement your solution using the cells below. Report your results and the process you followed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ccd1d12620f1a3e1c7b026b862056546",
     "grade": true,
     "grade_id": "cell-f44811f1e99ee41e",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "# param_grid = [{'criterion': [\"gini\", \"entropy\"],\n",
    "#                'max_depth': list(range(1, 45)),\n",
    "#                'max_leaf_nodes': [50, 150, 250, 350]\n",
    "#                }]\n",
    "#\n",
    "# grid_search = GridSearchCV(DecisionTreeClassifier(random_state = RANDOM_VARIABLE), param_grid, cv=10, scoring='f1', return_train_score=True, verbose=3)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# print(grid_search.best_params_)\n",
    "# CV Result: {'criterion': 'gini', 'max_depth': 14, 'max_leaf_nodes': 150}\n",
    "\n",
    "best_params = {'criterion': \"gini\", 'max_depth': 14, 'max_leaf_nodes': 150}\n",
    "\n",
    "# Using the CV best parameters\n",
    "classifier.set_params(**best_params)\n",
    "\n",
    "f1Errs = learning_curve(X_train,y_train,X_test,y_test,classifier,'f1')\n",
    "accErrs = learning_curve(X_train,y_train,X_test,y_test,classifier,'accuracy')\n",
    "\n",
    "# The final test f1 score for the classifier\n",
    "final_score = f1Errs[1][-1]\n",
    "print(final_score)\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b26041c1723d2858ad0833f8985801db",
     "grade": true,
     "grade_id": "cell-c99ca88f33f3717c",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A simple GridSearch Cross Validation is used to find the best fine-tuning parameters for the model, and an immediate\n",
    "impact is obvious when using the best parameters for the trained model. The final model has less variance with the\n",
    "tuned parameters and achieves a final f1 score of 0.5750767738807175.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}