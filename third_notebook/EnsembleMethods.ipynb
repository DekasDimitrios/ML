{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 3 - Ensemble Methods #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e378be0e0c4ed85ebc6bcc53e256aa5",
     "grade": false,
     "grade_id": "cell-17ca53188deb1a2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Welcome to your third assignment. This exercise will test your understanding on Ensemble Methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fb9c467676bec1973d8e90bb815e23f",
     "grade": false,
     "grade_id": "cell-1a33a1efbf02238c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Always run this cell\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# USE THE FOLLOWING RANDOM STATE FOR YOUR CODE\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa2d11566658cb0f6d5ea19212a619de",
     "grade": false,
     "grade_id": "cell-7210caf6b2891007",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Download the Dataset ##\n",
    "Download the dataset using the following cell or from this [link](https://github.com/sakrifor/public/tree/master/machine_learning_course/EnsembleDataset) and put the files in the same folder as the .ipynb file. \n",
    "In this assignment you are going to work with a dataset originated from the [ImageCLEFmed: The Medical Task 2016](https://www.imageclef.org/2016/medical) and the **Compound figure detection** subtask. The goal of this subtask is to identify whether a figure is a compound figure (one image consists of more than one figure) or not. The train dataset consits of 4197 examples/figures and each figure has 4096 features which were extracted using a deep neural network. The *CLASS* column represents the class of each example where 1 is a compoung figure and 0 is not. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c426c8680b4b28c9595139bec1d32f27",
     "grade": false,
     "grade_id": "cell-a413577b7685bfbe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url_train = 'https://github.com/sakrifor/public/raw/master/machine_learning_course/EnsembleDataset/train_set.csv'\n",
    "filename_train = 'train_set.csv'\n",
    "urllib.request.urlretrieve(url_train, filename_train)\n",
    "url_test = 'https://github.com/sakrifor/public/raw/master/machine_learning_course/EnsembleDataset/test_set_noclass.csv'\n",
    "filename_test = 'test_set_noclass.csv'\n",
    "urllib.request.urlretrieve(url_test, filename_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e073a9f617d5e2e192ef70d370f000b",
     "grade": false,
     "grade_id": "cell-cbadea205635117c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to load the data\n",
    "train_set = pd.read_csv(\"train_set.csv\").sample(frac=1).reset_index(drop=True)\n",
    "train_set.head()\n",
    "X = train_set.drop(columns=['CLASS'])\n",
    "y = train_set['CLASS'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5c0ed51001ec5b7e6cdcef1303473e7",
     "grade": false,
     "grade_id": "cell-4f509bca5cb87e84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.0 Testing different ensemble methods ##\n",
    "In this part of the assignment you are asked to create and test different ensemble methods using the train_set.csv dataset. You should use **10-fold cross validation** for your tests and report the average f-measure and accuracy of your models.\n",
    "\n",
    "### !!! Use n_jobs=-1 where is posibble to use all the cores of a machine for running your tests ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d56eafac9ce59f34521ab931a24e9c8",
     "grade": false,
     "grade_id": "cell-db7468662add40fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.1 Voting ###\n",
    "Create a voting classifier which uses three estimators/classifiers. Test both soft and hard voting and choose the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bbf11fd8382f22bddabe61416516e7be",
     "grade": true,
     "grade_id": "cell-3a1719cdb031d112",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:   22.7s remaining:   52.9s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:   24.9s remaining:   10.7s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   32.7s finished\n"
     ]
    }
   ],
   "source": [
    "# BEGIN CODE HERE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "# Preprocess the data by MinMax scaling\n",
    "transformer = MinMaxScaler().fit(X)\n",
    "X = pd.DataFrame(transformer.transform(X))\n",
    "y_train = pd.DataFrame(y)\n",
    "\n",
    "# Pick best parameters for classifier number 1\n",
    "# param_grid = [{'criterion': [\"gini\", \"entropy\"],\n",
    "#                'max_depth': list(range(1, 38)),\n",
    "#                'min_samples_split': [2, 3, 4, 5]\n",
    "#                }]\n",
    "#\n",
    "# grid_search = GridSearchCV(DecisionTreeClassifier(random_state = RANDOM_STATE), param_grid, cv=10, scoring='f1', return_train_score=True, verbose=3)\n",
    "# grid_search.fit(X, y)\n",
    "# print(grid_search.best_params_)\n",
    "# CV Result: {'criterion': 'entropy', 'max_depth': 4, 'min_samples_split': 2}\n",
    "\n",
    "# Using the CV best parameters for classifier number 1\n",
    "cls1 = DecisionTreeClassifier(random_state=RANDOM_STATE, criterion='entropy', max_depth=4, min_samples_split=2) # Classifier #1\n",
    "\n",
    "# Pick best parameters for classifier number 2\n",
    "# param_grid = [{'criterion': [\"gini\", \"entropy\"]}]\n",
    "#\n",
    "# grid_search = GridSearchCV(DecisionTreeClassifier(random_state = RANDOM_STATE), param_grid, cv=10, scoring='f1', return_train_score=True, verbose=3)\n",
    "# grid_search.fit(X, y)\n",
    "# print(grid_search.best_params_)\n",
    "# CV Result: {'criterion': 'entropy'}\n",
    "\n",
    "# Using the CV best parameters for classifier number 2\n",
    "cls2 = DecisionTreeClassifier(random_state=RANDOM_STATE, criterion='entropy') # Classifier #2\n",
    "\n",
    "# Pick best parameters for classifier number 3\n",
    "# param_grid = [{'n_neighbors': [3, 5],\n",
    "#                'weights': [\"uniform\", \"distance\"],\n",
    "#                'p': [1, 2] # L1 or L2\n",
    "#                }]\n",
    "#\n",
    "# grid_search = GridSearchCV(KNeighborsClassifier(n_jobs=-1), param_grid, cv=10, scoring='f1', return_train_score=True, verbose=3)\n",
    "# grid_search.fit(X, y)\n",
    "# print(grid_search.best_params_)\n",
    "# CV Result: {'n_neighbors': 5, 'p': 2, 'weights': 'distance'}\n",
    "\n",
    "# Using the CV best parameters for classifier number 3\n",
    "cls3 = KNeighborsClassifier(n_neighbors=5, weights='distance', p=2, n_jobs=-1) # Classifier #3\n",
    "\n",
    "clrs = [('dt4', cls1), ('dt', cls2), ('knn', cls3)]\n",
    "\n",
    "# Pick between hard and soft voting\n",
    "# param_grid = [{'voting': [\"hard\", \"soft\"]}]\n",
    "#\n",
    "# grid_search = GridSearchCV(VotingClassifier(estimators=clrs, n_jobs=-1), param_grid, cv=10, scoring='f1', return_train_score=True, verbose=3)\n",
    "# grid_search.fit(X, y)\n",
    "# print(grid_search.best_params_)\n",
    "# CV Result: {'voting': 'hard'}\n",
    "\n",
    "# # # # # This code would probably (can't be sure) produce better results, but it would take an eternity to execute # # # # #\n",
    "# cls1 = DecisionTreeClassifier(random_state=RANDOM_STATE, criterion='entropy', max_depth=4, min_samples_split=2) # Classifier #1\n",
    "# cls2 = DecisionTreeClassifier(random_state=RANDOM_STATE, criterion='entropy') # Classifier #2\n",
    "# cls3 = KNeighborsClassifier(n_neighbors=5, weights='distance', p=2, n_jobs=-1) # Classifier #3\n",
    "# clrs = [('dt', cls1), ('dta', cls2), ('knn', cls3)]\n",
    "# vcls = VotingClassifier(estimators=clrs, n_jobs=-1) # Voting Classifier\n",
    "#\n",
    "# param_grid = [{'dt__criterion': [\"gini\", \"entropy\"],\n",
    "#                'dt__max_depth': list(range(1, 38)),\n",
    "#                'dt__min_samples_split': [2, 3, 4, 5],\n",
    "#                'dta__criterion': [\"gini\", \"entropy\"],\n",
    "#                'knn__n_neighbors': [3, 5],\n",
    "#                'knn__weights': [\"uniform\", \"distance\"],\n",
    "#                'knn__p': [1, 2] # L1 or L2\n",
    "#                }]\n",
    "#\n",
    "# grid_search = GridSearchCV(estimator=vcls, param_grid=param_grid, cv=10, scoring='f1', return_train_score=True, verbose=3, n_jobs=-1)\n",
    "# grid_search.fit(X, y)\n",
    "# print(grid_search.best_params_)\n",
    "\n",
    "vcls = VotingClassifier(estimators=clrs, voting='hard', n_jobs=-1)\n",
    "\n",
    "scores = cross_validate(estimator=vcls, X=X, y=y, cv=10,\n",
    "                        scoring=['f1', 'accuracy'],\n",
    "                        return_train_score=True,\n",
    "                        verbose=3,\n",
    "                        n_jobs=-1)\n",
    "\n",
    "avg_fmeasure = np.average(scores['test_f1']) # The average f-measure\n",
    "avg_accuracy = np.average(scores['test_accuracy']) # The average accuracy\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f0e52e5eea2eb2cb23380c80ff846cf",
     "grade": false,
     "grade_id": "cell-0ef59e80595937ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:\n",
      "VotingClassifier(estimators=[('dt4',\n",
      "                              DecisionTreeClassifier(criterion='entropy',\n",
      "                                                     max_depth=4,\n",
      "                                                     random_state=42)),\n",
      "                             ('dt',\n",
      "                              DecisionTreeClassifier(criterion='entropy',\n",
      "                                                     random_state=42)),\n",
      "                             ('knn',\n",
      "                              KNeighborsClassifier(n_jobs=-1,\n",
      "                                                   weights='distance'))],\n",
      "                 n_jobs=-1)\n",
      "F1-Score:0.8115998157012017 & Accuracy:0.7638157745198317\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "print(vcls)\n",
    "print(\"F1-Score:{} & Accuracy:{}\".format(avg_fmeasure,avg_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c7e6ab511ff3a546d2e2efb5feb892f",
     "grade": false,
     "grade_id": "cell-f6d620a3fd102626",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2 Stacking ###\n",
    "Create a stacking classifier which uses two estimators/classifiers. Try different classifiers for the combination of the initial classifiers. Report your results in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc7e22a168b668cbd10c524297950133",
     "grade": true,
     "grade_id": "cell-2ae5e38bd546681e",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:   40.6s remaining:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:   41.3s remaining:   17.7s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   54.6s finished\n"
     ]
    }
   ],
   "source": [
    "# BEGIN CODE HERE\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# cls1 = DecisionTreeClassifier(random_state=RANDOM_STATE, criterion='entropy', max_depth=4, min_samples_split=2) # Classifier #1\n",
    "# cls2 = DecisionTreeClassifier(random_state=RANDOM_STATE, criterion='entropy') # Classifier #2\n",
    "# clrs = [('cls1', cls1), ('cls2', cls2)]\n",
    "# scls = StackingClassifier(estimators=clrs, n_jobs=-1, verbose=3) # Stacking Classifier\n",
    "#\n",
    "# scores = cross_validate(estimator=scls, X=X, y=y, cv=10,\n",
    "#                         scoring=['f1', 'accuracy'],\n",
    "#                         return_train_score=True,\n",
    "#                         verbose=3,\n",
    "#                         n_jobs=-1)\n",
    "#\n",
    "# avg_fmeasure = np.average(scores['test_f1']) # The average f-measure\n",
    "# avg_accuracy = np.average(scores['test_accuracy']) # The average accuracy\n",
    "\n",
    "cls1 = DecisionTreeClassifier(random_state=RANDOM_STATE, criterion='entropy', max_depth=4, min_samples_split=2) # Classifier #1\n",
    "cls2 = KNeighborsClassifier(n_neighbors=5, weights='distance', p=2, n_jobs=-1) # Classifier #3\n",
    "clrs = [('cls1', cls1), ('cls2', cls2)]\n",
    "scls = StackingClassifier(estimators=clrs, n_jobs=-1, verbose=3) # Stacking Classifier\n",
    "\n",
    "scores = cross_validate(estimator=scls, X=X, y=y, cv=10,\n",
    "                        scoring=['f1', 'accuracy'],\n",
    "                        return_train_score=True,\n",
    "                        verbose=3,\n",
    "                        n_jobs=-1)\n",
    "\n",
    "avg_fmeasure = np.average(scores['test_f1']) # The average f-measure\n",
    "avg_accuracy = np.average(scores['test_accuracy'])\n",
    "\n",
    "# cls1 = DecisionTreeClassifier(random_state=RANDOM_STATE, criterion='entropy') # Classifier #2\n",
    "# cls2 = KNeighborsClassifier(n_neighbors=5, weights='distance', p=2, n_jobs=-1) # Classifier #3\n",
    "# clrs = [('cls1', cls1), ('cls2', cls2)]\n",
    "# scls = StackingClassifier(estimators=clrs, n_jobs=-1, verbose=3) # Stacking Classifier\n",
    "#\n",
    "# scores = cross_validate(estimator=scls, X=X, y=y, cv=10,\n",
    "#                         scoring=['f1', 'accuracy'],\n",
    "#                         return_train_score=True,\n",
    "#                         verbose=3,\n",
    "#                         n_jobs=-1)\n",
    "#\n",
    "# avg_fmeasure = np.average(scores['test_f1']) # The average f-measure\n",
    "# avg_accuracy = np.average(scores['test_accuracy'])\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a27a3122627aed7d5a6f5678055f712",
     "grade": false,
     "grade_id": "cell-6d6cadab378a2b03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:\n",
      "StackingClassifier(estimators=[('cls1',\n",
      "                                DecisionTreeClassifier(criterion='entropy',\n",
      "                                                       max_depth=4,\n",
      "                                                       random_state=42)),\n",
      "                               ('cls2',\n",
      "                                KNeighborsClassifier(n_jobs=-1,\n",
      "                                                     weights='distance'))],\n",
      "                   n_jobs=-1, verbose=3)\n",
      "F1-Score:0.8309012150241951 & Accuracy:0.7950448914649393\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "print(scls)\n",
    "print(\"F1-Score:{} & Accuracy:{}\".format(avg_fmeasure,avg_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6b0c745a12e384341f4e246c5242d25",
     "grade": false,
     "grade_id": "cell-8a05446ba9a944c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.3 Report the results ###\n",
    "Report the results of your experiments in the following cell. How did you choose your initial classifiers? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e3da261e2e18ede4c057e21e080b9bac",
     "grade": true,
     "grade_id": "cell-1522ee0b7c414fba",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Firstly, a basic MinMaxScaler is used to preprocess the data.\n",
    "\n",
    "For the selection of classifiers the following strategy is used, in an attempt to produce reasonably good results.\n",
    "\n",
    "SVMs seem unreasonable to use in ensembles due to the long training time, and the high scores they offer.\n",
    "Probably best to use them by themselves.\n",
    "\n",
    "Gaussian Classifiers seem to not offer good enough scores in comparison to other classification algorithms.\n",
    "\n",
    "Use the DecisionTreeClassifier that produces the best score.\n",
    "Use the DecisionTreeClassifier that is fully adapted to the training data.\n",
    "Use the KNeighborsClassifier in order to capture the spatial information that characterizes the data without the\n",
    "overfitting nature that follows the fully adapted DecisionTreeClassifier.\n",
    "\n",
    "After the above process is done, the hard vs soft voting dilemma is solved.\n",
    "Finally, the voting classifier is trained, and the predictions are made.\n",
    "The above method results in a f1-score approximating 0.81 and accuracy roughly equal to 0.76.\n",
    "\n",
    "For the stacking classifier, all possible combinations of the three classifiers that were used previously are tried and\n",
    "the one that grants us with the best results was kept. It seems a quite reasonable strategy.\n",
    "\n",
    "The above process resulted in a f1-score approximating 0.83 and accuracy roughly equal to 0.79 when using KNeighborsClassifier\n",
    "combined with the best-scoring DecisionTreeClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7520f22a6a708d14fa6d56d42b78d9f",
     "grade": false,
     "grade_id": "cell-b40c3a7c4ef32588",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.0 Randomization ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5073f6ead7355190904470c661d86d53",
     "grade": false,
     "grade_id": "cell-64c9c6881b26f5bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**2.1** You are asked to create three ensembles of decision trees where each one uses a different method for producing homogeneous ensembles. Compare them with a simple decision tree classifier and report your results in the dictionaries (dict) below using as key the given name of your classifier and as value the f1/accuracy score. The dictionaries should contain four different elements.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1661b594e02f8f8a73ceaad8f03b85a3",
     "grade": true,
     "grade_id": "cell-9e760b938516b506",
     "locked": false,
     "points": 30,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   7 out of  10 | elapsed:  4.4min remaining:  1.9min\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:  5.5min finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   7 out of  10 | elapsed:    3.8s remaining:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    5.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   7 out of  10 | elapsed:  7.8min remaining:  3.3min\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed: 10.7min finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   7 out of  10 | elapsed:    4.7s remaining:    2.0s\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    6.7s finished\n"
     ]
    }
   ],
   "source": [
    "# BEGIN CODE HERE\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# ens1 = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=RANDOM_STATE), n_estimators=100, random_state=RANDOM_STATE, n_jobs=4, verbose=3)\n",
    "# ens1_scores = cross_validate(estimator=ens1, X=X, y=y, cv=10,\n",
    "#                         scoring=['f1', 'accuracy'],\n",
    "#                         return_train_score=True,\n",
    "#                         verbose=3,\n",
    "#                         n_jobs=4)\n",
    "#\n",
    "# ens2 = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=4, verbose=3)\n",
    "# ens2_scores = cross_validate(estimator=ens2, X=X, y=y, cv=10,\n",
    "#                         scoring=['f1', 'accuracy'],\n",
    "#                         return_train_score=True,\n",
    "#                         verbose=3,\n",
    "#                         n_jobs=4)\n",
    "#\n",
    "# ens3 = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(random_state=RANDOM_STATE), n_estimators=100, random_state=RANDOM_STATE)\n",
    "# ens3_scores = cross_validate(estimator=ens3, X=X, y=y, cv=10,\n",
    "#                         scoring=['f1', 'accuracy'],\n",
    "#                         return_train_score=True,\n",
    "#                         verbose=3,\n",
    "#                         n_jobs=4)\n",
    "#\n",
    "# tree = DecisionTreeClassifier(random_state=RANDOM_STATE) # Classifier #1\n",
    "# tree_scores = cross_validate(estimator=tree, X=X, y=y, cv=10,\n",
    "#                         scoring=['f1', 'accuracy'],\n",
    "#                         return_train_score=True,\n",
    "#                         verbose=3,\n",
    "#                         n_jobs=4)\n",
    "#\n",
    "# f_measures = {'Ensemble with Bagging': np.average(ens1_scores['test_f1']),\n",
    "#               'Ensemble with Random Forest': np.average(ens2_scores['test_f1']),\n",
    "#               'Ensemble with AdaBoost': np.average(ens3_scores['test_f1']),\n",
    "#               'Simple Decision': np.average(tree_scores['test_f1'])}\n",
    "# accuracies = {'Ensemble with Bagging': np.average(ens1_scores['test_accuracy']),\n",
    "#               'Ensemble with Random Forest': np.average(ens2_scores['test_accuracy']),\n",
    "#               'Ensemble with AdaBoost': np.average(ens3_scores['test_accuracy']),\n",
    "#               'Simple Decision': np.average(tree_scores['test_accuracy'])}\n",
    "# Example f_measures = {'Simple Decision':0.8551, 'Ensemble with random ...': 0.92, ...}\n",
    "\n",
    "bcls = DecisionTreeClassifier(random_state=RANDOM_STATE, criterion='entropy', max_depth=4, min_samples_split=2) # Classifier #1\n",
    "\n",
    "ens1 = BaggingClassifier(base_estimator=bcls, n_estimators=100, random_state=RANDOM_STATE, n_jobs=4, verbose=3)\n",
    "ens1_scores = cross_validate(estimator=ens1, X=X, y=y, cv=10,\n",
    "                        scoring=['f1', 'accuracy'],\n",
    "                        return_train_score=True,\n",
    "                        verbose=3,\n",
    "                        n_jobs=4)\n",
    "\n",
    "ens2 = RandomForestClassifier(criterion='entropy', max_depth=4, min_samples_split=2, n_estimators=100, random_state=RANDOM_STATE, n_jobs=4, verbose=3)\n",
    "ens2_scores = cross_validate(estimator=ens2, X=X, y=y, cv=10,\n",
    "                        scoring=['f1', 'accuracy'],\n",
    "                        return_train_score=True,\n",
    "                        verbose=3,\n",
    "                        n_jobs=4)\n",
    "\n",
    "ens3 = AdaBoostClassifier(base_estimator=bcls, n_estimators=100, random_state=RANDOM_STATE)\n",
    "ens3_scores = cross_validate(estimator=ens3, X=X, y=y, cv=10,\n",
    "                        scoring=['f1', 'accuracy'],\n",
    "                        return_train_score=True,\n",
    "                        verbose=3,\n",
    "                        n_jobs=4)\n",
    "\n",
    "tree = bcls\n",
    "tree_scores = cross_validate(estimator=tree, X=X, y=y, cv=10,\n",
    "                        scoring=['f1', 'accuracy'],\n",
    "                        return_train_score=True,\n",
    "                        verbose=3,\n",
    "                        n_jobs=4)\n",
    "\n",
    "f_measures = {'Ensemble with Bagging': np.average(ens1_scores['test_f1']),\n",
    "              'Ensemble with Random Forest': np.average(ens2_scores['test_f1']),\n",
    "              'Ensemble with AdaBoost': np.average(ens3_scores['test_f1']),\n",
    "              'Simple Decision': np.average(tree_scores['test_f1'])}\n",
    "accuracies = {'Ensemble with Bagging': np.average(ens1_scores['test_accuracy']),\n",
    "              'Ensemble with Random Forest': np.average(ens2_scores['test_accuracy']),\n",
    "              'Ensemble with AdaBoost': np.average(ens3_scores['test_accuracy']),\n",
    "              'Simple Decision': np.average(tree_scores['test_accuracy'])}\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0da6aad44e9bd8a9f7cc06eccec886c0",
     "grade": false,
     "grade_id": "cell-77f4dc2cd4cb2f7e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy',\n",
      "                                                        max_depth=4,\n",
      "                                                        random_state=42),\n",
      "                  n_estimators=100, n_jobs=4, random_state=42, verbose=3)\n",
      "RandomForestClassifier(criterion='entropy', max_depth=4, n_jobs=4,\n",
      "                       random_state=42, verbose=3)\n",
      "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy',\n",
      "                                                         max_depth=4,\n",
      "                                                         random_state=42),\n",
      "                   n_estimators=100, random_state=42)\n",
      "DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=42)\n",
      "Classifier:Ensemble with Bagging -  F1:0.80528595843536\n",
      "Classifier:Ensemble with Random Forest -  F1:0.800471607847261\n",
      "Classifier:Ensemble with AdaBoost -  F1:0.8034443552382895\n",
      "Classifier:Simple Decision -  F1:0.7760530031602773\n",
      "Classifier:Ensemble with Bagging -  Accuracy:0.7492857142857143\n",
      "Classifier:Ensemble with Random Forest -  Accuracy:0.7256909876122287\n",
      "Classifier:Ensemble with AdaBoost -  Accuracy:0.7659699965905216\n",
      "Classifier:Simple Decision -  Accuracy:0.7075769973860666\n"
     ]
    }
   ],
   "source": [
    "print(ens1)\n",
    "print(ens2)\n",
    "print(ens3)\n",
    "print(tree)\n",
    "for name,score in f_measures.items():\n",
    "    print(\"Classifier:{} -  F1:{}\".format(name,score))\n",
    "for name,score in accuracies.items():\n",
    "    print(\"Classifier:{} -  Accuracy:{}\".format(name,score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48f3eafac80f6dd15441c4991c78836d",
     "grade": false,
     "grade_id": "cell-a6ea07f0be814a40",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**2.2** Describe your classifiers and your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9395efc3b936166e55b3bec6e0afdab3",
     "grade": true,
     "grade_id": "cell-399fc5e7254f1c58",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "At first, the three different methods for producing homogeneous ensembles (Bagging, Random Forest, Boosting), using 100\n",
    "estimators, are compared with each other and also with a simple Decision Tree Classifier, without fine-tuning the\n",
    "algorithm parameters. The results are presented in the table below. Bagging and Random Forest methodologies far\n",
    "outperform the Boosting algorithm and the Simple Tree.\n",
    "\n",
    "| Algorithm     |      F1 Score      |   Accuracy Score   |\n",
    "|---------------|:------------------:|:------------------:|\n",
    "| Bagging       | 0.8479842034261897 | 0.8141072849187407 |\n",
    "| Random Forest | 0.8475386243179933 | 0.8102932151380839 |\n",
    "| Boosting      | 0.7370851894386801 | 0.6928037276963291 |\n",
    "| Simple Tree   | 0.7393819455067371 | 0.6944635754062961 |\n",
    "\n",
    "In the second experiment, the above process is followed, but the Decision Tree used is the fine-tuned one. The results\n",
    "show that when the tree used is the one that produces the best score, the difference between the different methods for\n",
    "producing homogeneous ensembles are minimal, but still outperforming the simple tree. Boosting is greatly benefited by\n",
    "using a fine-tuned model since it staring point is far better than the one when using the tree without the right\n",
    "parameters. Bagging and Random Forest are not performing quite well when the fine-tuned model is used due to the fact\n",
    "that they have no ability to exploit partial adaptation to the data since the best scoring tree has limited depth.\n",
    "\n",
    "| Algorithm     |      F1 Score      |   Accuracy Score   |\n",
    "|---------------|:------------------:|:------------------:|\n",
    "| Bagging       | 0.8088751928182969 | 0.7540373906125696 |\n",
    "| Random Forest |  0.80231569382201  |  0.728073644732356 |\n",
    "| Boosting      |  0.806041713353633 | 0.7683509489714739 |\n",
    "| Simple Tree   | 0.7609822052165569 | 0.7056665530173882 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8edad6471eab5b0645f6ca46bab2f7a",
     "grade": false,
     "grade_id": "cell-a0de461bc76e0880",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**2.3** Increasing the number of estimators in a bagging classifier can drastically increase the training time of a classifier. Is there any solution to this problem? Can the same solution be applied to boosting classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9573961dbe26d9ce6669df5da70a45a",
     "grade": true,
     "grade_id": "cell-0a28025407c78a48",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The main solution for the problem is called parallelization. Using the n_jobs parameter of sklearn's method allows for\n",
    "n number of jobs to run in parallel for both fit and predict. That way both training and predicting are tremendously\n",
    "sped up. However, that parameter cannot be used for boosting classifiers since there is a linear connection between\n",
    "each and every estimator running. The first classifiers need to be trained in order for the second one to start its own\n",
    "training process. Due to that fact, its apparent that parallelization cannot be used for this type of ensembles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d016d23320010c4d8d9e8218cba553e8",
     "grade": false,
     "grade_id": "cell-35e46873d8c6537c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3.0 Creating the best classifier ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e7e2a127a27e4c9131e94ae73e6e325b",
     "grade": false,
     "grade_id": "cell-6de6582e696ba2d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**3.1** In this part of the assignment you are asked to train the best possible ensemble! Describe the process you followed to achieve this result. How did you choose your classifier and your parameters and why. Report the f-measure & accuracy (10-fold cross validation) of your final classifier and results of classifiers you tried in the cell following the code. Can you achieve an accuracy over 83-84%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8d58b62ea96a7e0e0776f79c58e4b10",
     "grade": true,
     "grade_id": "cell-d1bba508731c9030",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed: 16.2min finished\n"
     ]
    }
   ],
   "source": [
    "# BEGIN CODE HERE\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "\n",
    "best_cls = GradientBoostingClassifier(random_state=RANDOM_STATE, verbose=4, n_estimators=100, learning_rate=0.5, max_depth=9, min_samples_split=3)\n",
    "\n",
    "# param_grid = [{'learning_rate': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "#                'max_depth': list(range(1, 10)),\n",
    "#                'min_samples_split': list(range(1, 5))\n",
    "#                }]\n",
    "#\n",
    "# grid_search = GridSearchCV(estimator=best_cls, param_grid=param_grid, cv=3, scoring='accuracy', return_train_score=True, verbose=3, n_jobs=6)\n",
    "# grid_search.fit(X, y)\n",
    "# print(grid_search.best_params_)\n",
    "# CV Result: {'max_depth': 4, 'min_samples_split': 3, 'learning_rate': 0.3}\n",
    "\n",
    "scores = cross_validate(estimator=best_cls, X=X, y=y, cv=10,\n",
    "                        scoring=['f1', 'accuracy'],\n",
    "                        return_train_score=True,\n",
    "                        n_jobs=4,\n",
    "                        verbose=2)\n",
    "\n",
    "best_fmeasure = np.average(scores['test_f1']) # The average f-measure\n",
    "best_accuracy = np.average(scores['test_accuracy'])\n",
    "\n",
    "# END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ecc691ce956fad47c497b9849747c34",
     "grade": false,
     "grade_id": "cell-39673f451b660dcc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:\n",
      "GradientBoostingClassifier(learning_rate=0.5, max_depth=9, min_samples_split=3,\n",
      "                           random_state=42, verbose=4)\n",
      "F1-Score:0.8649371747864608 & Accuracy:0.8364996022275258\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "print(best_cls)\n",
    "print(\"F1-Score:{} & Accuracy:{}\".format(best_fmeasure,best_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "667632db5afb6f143062507bae31063f",
     "grade": false,
     "grade_id": "cell-6a072817c64ce4a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**3.2** Describe the process you followed to achieve this result. How did you choose your classifier and your parameters and why. Report the f-measure & accuracy (10-fold cross validation) of your final classifier and results of classifiers you tried in the cell following the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3abcb52a70fcb6da4f93980026b8e593",
     "grade": true,
     "grade_id": "cell-5f1d5ba45ffeb074",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "After trying different ensemble combinations, limited to using 100 estimators, the one that showed the most promising\n",
    "results was the one using Gradient Boosting after fine-tuning its parameters using 10-Fold cross validation. Increasing\n",
    "the number of estimator to 250 can result in an improvement of the models accuracy and f1-score, but a constant number\n",
    "of 100 estimators was kept in order to make the ensemble comparison an easier process.\n",
    "\n",
    "| Gradient Boosting Parameters |      F1 Score      |   Accuracy Score   |\n",
    "|---------------|:------------------:|:------------------:|\n",
    "| learning_rate=0.5, max_depth=9, min_samples_split=3, n_estimators=100 | 0.8649371747864608 | 0.8364996022275258 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22500bda285c8dc9375ee048e883be55",
     "grade": false,
     "grade_id": "cell-5b27d068d1fbfa37",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**3.3** Create a classifier that is going to be used in production - in a live system. Use the *test_set_noclass.csv* to make predictions. Store the predictions in a list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c40bf6a2d6e630b217742246c20d2560",
     "grade": true,
     "grade_id": "cell-ab69a2863e87fd72",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.8308            4.17m\n",
      "         2           0.5678            4.18m\n",
      "         3           0.4032            4.19m\n",
      "         4           0.2938            4.23m\n",
      "         5           0.2215            4.50m\n",
      "         6           0.1801            4.61m\n",
      "         7           0.1482            4.65m\n",
      "         8           0.1200            4.58m\n",
      "         9           0.1028            4.48m\n",
      "        10           0.0877            4.39m\n",
      "        11           0.0771            4.30m\n",
      "        12           0.0674            4.23m\n",
      "        13           0.0594            4.17m\n",
      "        14           0.0536            4.10m\n",
      "        15           0.0473            4.03m\n",
      "        16           0.0416            3.97m\n",
      "        17           0.0379            3.91m\n",
      "        18           0.0345            3.85m\n",
      "        19           0.0322            3.79m\n",
      "        20           0.0287            3.75m\n",
      "        21           0.0260            3.73m\n",
      "        22           0.0230            3.67m\n",
      "        23           0.0206            3.62m\n",
      "        24           0.0190            3.56m\n",
      "        25           0.0175            3.51m\n",
      "        26           0.0161            3.46m\n",
      "        27           0.0140            3.41m\n",
      "        28           0.0127            3.37m\n",
      "        29           0.0115            3.33m\n",
      "        30           0.0110            3.29m\n",
      "        31           0.0101            3.26m\n",
      "        32           0.0095            3.21m\n",
      "        33           0.0089            3.16m\n",
      "        34           0.0079            3.10m\n",
      "        35           0.0074            3.05m\n",
      "        36           0.0065            3.00m\n",
      "        37           0.0057            2.95m\n",
      "        38           0.0053            2.90m\n",
      "        39           0.0049            2.85m\n",
      "        40           0.0045            2.80m\n",
      "        41           0.0041            2.75m\n",
      "        42           0.0037            2.70m\n",
      "        43           0.0035            2.65m\n",
      "        44           0.0033            2.60m\n",
      "        45           0.0031            2.55m\n",
      "        46           0.0029            2.50m\n",
      "        47           0.0026            2.45m\n",
      "        48           0.0023            2.41m\n",
      "        49           0.0021            2.36m\n",
      "        50           0.0020            2.31m\n",
      "        51           0.0018            2.27m\n",
      "        52           0.0016            2.22m\n",
      "        53           0.0015            2.17m\n",
      "        54           0.0014            2.12m\n",
      "        55           0.0012            2.07m\n",
      "        56           0.0011            2.03m\n",
      "        57           0.0010            1.98m\n",
      "        58           0.0010            1.93m\n",
      "        59           0.0009            1.89m\n",
      "        60           0.0008            1.84m\n",
      "        61           0.0008            1.80m\n",
      "        62           0.0007            1.75m\n",
      "        63           0.0007            1.70m\n",
      "        64           0.0006            1.66m\n",
      "        65           0.0005            1.61m\n",
      "        66           0.0005            1.56m\n",
      "        67           0.0004            1.52m\n",
      "        68           0.0004            1.47m\n",
      "        69           0.0004            1.43m\n",
      "        70           0.0004            1.38m\n",
      "        71           0.0003            1.34m\n",
      "        72           0.0003            1.29m\n",
      "        73           0.0003            1.24m\n",
      "        74           0.0003            1.20m\n",
      "        75           0.0002            1.16m\n",
      "        76           0.0002            1.12m\n",
      "        77           0.0002            1.07m\n",
      "        78           0.0002            1.03m\n",
      "        79           0.0002           58.86s\n",
      "        80           0.0002           56.04s\n",
      "        81           0.0001           53.23s\n",
      "        82           0.0001           50.42s\n",
      "        83           0.0001           47.62s\n",
      "        84           0.0001           44.82s\n",
      "        85           0.0001           42.02s\n",
      "        86           0.0001           39.22s\n",
      "        87           0.0001           36.42s\n",
      "        88           0.0001           33.60s\n",
      "        89           0.0001           30.81s\n",
      "        90           0.0001           27.99s\n",
      "        91           0.0001           25.19s\n",
      "        92           0.0001           22.39s\n",
      "        93           0.0001           19.59s\n",
      "        94           0.0001           16.80s\n",
      "        95           0.0001           14.00s\n",
      "        96           0.0001           11.20s\n",
      "        97           0.0000            8.40s\n",
      "        98           0.0000            5.60s\n",
      "        99           0.0000            2.81s\n",
      "       100           0.0000            0.00s\n"
     ]
    }
   ],
   "source": [
    "# BEGIN CODE HERE\n",
    "cls = best_cls.fit(X,y)\n",
    "X_test = pd.DataFrame(transformer.transform(pd.read_csv(\"test_set_noclass.csv\")))\n",
    "predictions = best_cls.predict(X_test)\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ace9dbe06e5607ddf9353befef8472c0",
     "grade": false,
     "grade_id": "cell-d98d6687c3bbe4ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(learning_rate=0.5, max_depth=9, min_samples_split=3,\n",
      "                           random_state=42, verbose=4)\n",
      "[1 0 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(cls)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1af1d441fd486d53e45173d4f352ba8c",
     "grade": false,
     "grade_id": "cell-966633c679d5c960",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "LEAVE HERE ANY COMMENTS ABOUT YOUR CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24dbe2151df25b6e5b36e988a8e38dcb",
     "grade": false,
     "grade_id": "cell-78ffc0c68225fb1a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### This following cell will not be executed. The test_set.csv with the classes will be made available after the deadline and this cell is for testing purposes!!! Do not modify it! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddcf51aaeaaa305540873fd0012a4b06",
     "grade": false,
     "grade_id": "cell-7946d9ee342bf549",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "final_test_set = pd.read_csv('test_set.csv')\n",
    "ground_truth = final_test_set['CLASS']\n",
    "print(\"Accuracy:{}\".format(accuracy_score(predictions,ground_truth)))\n",
    "print(\"F1-Score:{}\".format(f1_score(predictions,ground_truth)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}